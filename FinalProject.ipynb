{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e13ab26",
   "metadata": {},
   "source": [
    "# Airbnb Data Analysis for New York City\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Airbnb, an online marketplace for lodging, has transformed the way people travel and find accommodations. In major cities like New York City, Airbnb listings provide a wide variety of options for travelers, ranging from entire apartments and homes to private rooms in shared apartments. This flexibility has made Airbnb a popular choice among both tourists and business travelers.\n",
    "\n",
    "In this notebook, we will explore the Airbnb dataset for New York City. This dataset provides detailed information on listings available on Airbnb, including prices, locations, types of properties, and reviews. By analyzing this data, we can gain insights into the rental market in New York City, understand pricing strategies, identify popular neighborhoods, and much more.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The dataset used in this analysis is obtained from [Inside Airbnb](http://insideairbnb.com/get-the-data.html), a website that provides publicly available data on Airbnb listings. The New York City dataset contains various attributes for each listing, including:\n",
    "\n",
    "- **Listing ID**: A unique identifier for each Airbnb listing.\n",
    "- **Name**: The name of the listing.\n",
    "- **Host ID**: A unique identifier for the host.\n",
    "- **Host Name**: The name of the host.\n",
    "- **Neighborhood Group**: The general area or borough where the listing is located (e.g., Manhattan, Brooklyn).\n",
    "- **Neighborhood**: The specific neighborhood within the borough.\n",
    "- **Latitude**: The latitude coordinate of the listing.\n",
    "- **Longitude**: The longitude coordinate of the listing.\n",
    "- **Room Type**: The type of room being offered (e.g., entire home/apt, private room, shared room).\n",
    "- **Price**: The price per night for the listing.\n",
    "- **Minimum Nights**: The minimum number of nights a guest must stay.\n",
    "- **Number of Reviews**: The total number of reviews for the listing.\n",
    "- **Last Review**: The date of the last review.\n",
    "- **Reviews per Month**: The average number of reviews per month.\n",
    "- **Calculated Host Listings Count**: The total number of listings by the host.\n",
    "- **Availability 365**: The number of days the listing is available in a year.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this analysis, we aim to achieve the following objectives:\n",
    "\n",
    "1. **Data Exploration**: Understand the structure and contents of the dataset through summary statistics and visualizations.\n",
    "2. **Price Analysis**: Analyze the pricing strategies of different types of listings and identify factors influencing prices.\n",
    "3. **Geographical Analysis**: Examine the geographical distribution of listings and identify popular neighborhoods.\n",
    "4. **Review Analysis**: Investigate the review patterns and their correlation with listing popularity and price.\n",
    "5. **Availability Analysis**: Analyze the availability of listings and identify trends related to booking frequency.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c2124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e37c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "df = pd.read_csv(\"data/listings.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f9ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7b7e4",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13318e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'last_review' column\n",
    "df.drop(columns=['last_review'], inplace=True)\n",
    "df.drop(columns=['license'], inplace=True)\n",
    "df.drop(columns=['host_id'], inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c6839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values per column before cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print('Shape of data before cleaning')\n",
    "print(df.shape)\n",
    "\n",
    "print('Number of unique neighborhoods before cleaning')\n",
    "print(df['neighbourhood'].nunique())\n",
    "\n",
    "# Visualize missing data\n",
    "plt.figure(figsize=(12, 6))\n",
    "msno.matrix(df)\n",
    "plt.show()\n",
    "\n",
    "# Visualize the missing data as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "msno.heatmap(df, cmap='coolwarm')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97702a6e",
   "metadata": {},
   "source": [
    "## Removing rows with missing data\n",
    "\n",
    "Find which neighborhoods are removed from the data after cleaning and add back those rows with the mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db11892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up whitespace or empty strings\n",
    "df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "# Store the original unique neighborhoods\n",
    "original_neighbourhoods = df['neighbourhood'].unique()\n",
    "\n",
    "# Remove rows with any missing data\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Check for missing values after dropping rows with missing data\n",
    "print(\"Missing values per column after cleaning:\")\n",
    "print(df_cleaned.isnull().sum())\n",
    "\n",
    "print('Shape of data after cleaning:')\n",
    "print(df_cleaned.shape)\n",
    "\n",
    "print('Number of unique neighborhoods after cleaning:')\n",
    "print(df_cleaned['neighbourhood'].nunique())\n",
    "\n",
    "# Verify there are no more missing values with matrix plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "msno.matrix(df_cleaned)\n",
    "plt.show()\n",
    "\n",
    "# Find which neighborhoods were removed\n",
    "cleaned_neighbourhoods = df_cleaned['neighbourhood'].unique()\n",
    "removed_neighbourhoods = set(original_neighbourhoods) - set(cleaned_neighbourhoods)\n",
    "print(f\"Removed neighborhoods: {removed_neighbourhoods}\")\n",
    "\n",
    "# Calculate mean values for numerical columns in the cleaned dataset\n",
    "mean_values = df_cleaned.mean(numeric_only=True)\n",
    "\n",
    "# Create a DataFrame for the removed neighborhoods with mean values\n",
    "rows_to_add = []\n",
    "for neighborhood in removed_neighbourhoods:\n",
    "    mean_row = mean_values.copy()\n",
    "    mean_row['neighbourhood'] = neighborhood\n",
    "    rows_to_add.append(mean_row)\n",
    "\n",
    "# Convert the list of Series to a DataFrame\n",
    "mean_rows_df = pd.DataFrame(rows_to_add)\n",
    "\n",
    "# Append the new rows to the cleaned DataFrame\n",
    "df_cleaned = pd.concat([df_cleaned, mean_rows_df], ignore_index=True)\n",
    "\n",
    "# Verify the updated dataset\n",
    "print(\"Data after adding back removed neighborhoods:\")\n",
    "print(df_cleaned.head())\n",
    "print(\"Missing values per column after adding back removed neighborhoods:\")\n",
    "print(df_cleaned.isnull().sum())\n",
    "\n",
    "# Display the first few rows after adding back the neighborhoods\n",
    "print(df_cleaned.head())\n",
    "\n",
    "# Verify the total number of unique neighborhoods\n",
    "print(f\"Total number of unique neighborhoods after adding back: {df_cleaned['neighbourhood'].nunique()}\")\n",
    "\n",
    "# Updating df with df_cleaned\n",
    "df = df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da49689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set styles for the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "# Histogram for price\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['price'])\n",
    "plt.title('Distribution of Airbnb Prices in NYC')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0, df['price'].quantile(0.99))  # To limit the x-axis to the 99th percentile to avoid extreme outliers\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e20b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into two groups\n",
    "df_below_1000 = df[df['price'] <= 1000]\n",
    "df_above_1000 = df[df['price'] > 1000]\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Scatter plot for prices <= 1000\n",
    "sc = plt.scatter(df_below_1000['longitude'], df_below_1000['latitude'], c=df_below_1000['price'], cmap='viridis', alpha=0.5, label='Price <= 1000')\n",
    "\n",
    "# Scatter plot for prices > 1000\n",
    "plt.scatter(df_above_1000['longitude'], df_above_1000['latitude'], color='red', alpha=0.5, label='Price > 1000')\n",
    "\n",
    "# Add color bar for the price <= 1000 points\n",
    "cbar = plt.colorbar(sc, label='Price (<= 1000)')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Airbnb Prices in NYC')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2ec5fa",
   "metadata": {},
   "source": [
    "The plot above shows the scatter plot of the price vs latitude-longitude. The prices above $1000 are marked by red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5207a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "# Count the occurrences of each room type\n",
    "room_type_counts = df['room_type'].value_counts()\n",
    "\n",
    "# Get the viridis colormap\n",
    "cmap = cm.get_cmap('viridis')\n",
    "\n",
    "# Normalize the color range\n",
    "norm = plt.Normalize(room_type_counts.min(), room_type_counts.max())\n",
    "colors = cmap(norm(room_type_counts.values))\n",
    "\n",
    "# Plot the counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(room_type_counts.index, room_type_counts.values, color=colors)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Count of Room Types in Airbnb Listings')\n",
    "plt.xlabel('Room Type')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add value labels on the bars\n",
    "for index, value in enumerate(room_type_counts.values):\n",
    "    plt.text(index, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a98b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each neighborhood group\n",
    "neighborhood_group_counts = df['neighbourhood_group'].value_counts()\n",
    "\n",
    "# Plot the counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(neighborhood_group_counts.index, neighborhood_group_counts.values, color='skyblue')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Count of Neighborhood Groups in Airbnb Listings')\n",
    "plt.xlabel('Neighborhood Group')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add value labels on the bars\n",
    "for index, value in enumerate(neighborhood_group_counts.values):\n",
    "    plt.text(index, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ac798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram for availability_365\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['availability_365'], bins=30, edgecolor='black', color='skyblue')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Histogram of Availability (365 days) in Airbnb Listings')\n",
    "plt.xlabel('Availability in 365 days')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f10fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eca960",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16788bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Categorical columns: {categorical_columns}\")\n",
    "\n",
    "# Encode categorical variables\n",
    "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Ensure all data is numeric\n",
    "print(df.dtypes)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3253c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 10],\n",
    "    'min_samples_leaf': [1, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    verbose=3  # Set verbosity to show progress\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Train model with best parameters\n",
    "model = DecisionTreeRegressor(**best_params, random_state=42)\n",
    "model.fit(X_train, y_train)  # Fit the model\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'R-squared: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd538ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 10],\n",
    "    'min_samples_leaf': [1, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    verbose=3  # Set verbosity to show progress\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Train model with best parameters\n",
    "model = RandomForestRegressor(**best_params, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {mse**0.5}')\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b1dd3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c166156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
